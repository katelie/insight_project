{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; float:center}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"<style>.container { width:100% !important; float:center}</style>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import re\n",
    "from collections import Counter\n",
    "import datetime\n",
    "from itertools import groupby\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as skl\n",
    "# import scipy as sp\n",
    "# import scipy.stats as sps\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import wordcloud as wc\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "# from nltk import Text\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set_context('notebook')\n",
    "sns.set_style('ticks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stops = set(stopwords.words('english'))\n",
    "    # using a set will make it faster to run through...\n",
    "\n",
    "punctuation = ['.',',',':','!',';','-','?','\"',\"'\",'(',')','—']   \n",
    "other = ['ive','ve', \"i've\", \"i'v\", 'i’ll', 'i’ve', 'i’v']  # 'deb','hideb','don','didn','twaittry','doesn','thank','heydeb',\n",
    "    \n",
    "stops_punc = set(stopwords.words('english') + punctuation)\n",
    "\n",
    "mystops = stopwords.words('english') + punctuation + other\n",
    "mystops_set = set(stopwords.words('english') + punctuation + other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize_sentences(comment):\n",
    "    return nltk.sent_tokenize(remove_newlines(comment.lower()))\n",
    "    \n",
    "    \n",
    "# def separate_sentences(frame, identifier, paragraph, how='merge'):\n",
    "#     sentences = pd.concat([pd.Series(row[identifier], tokenize_sentences((row[paragraph]))) for _, row in frame.iterrows()]).reset_index()\n",
    "#     sentences.columns = ['sentence', identifier]\n",
    "    \n",
    "#     if how == 'merge':\n",
    "#         return sentences.merge(frame, left_on=identifier, right_on=identifier, how='outer', sort=False, suffixes=('_l','_r')) \n",
    "#     elif how == 'nomerge': \n",
    "#         return sentences\n",
    "#     else: \n",
    "#         return sentences.merge(frame, left_on=identifier, right_on=identifier, how='outer', sort=False, suffixes=('_l','_r')) \n",
    "\n",
    "# slightly faster version\n",
    "def separate_sentences(frame, identifier, paragraph, how='merge'):\n",
    "    sentences = pd.DataFrame((tokenize_sentences(row[paragraph]) for _, row in frame.iterrows()), index=frame[identifier]).stack()\n",
    "    sentences = sentences.reset_index()[[0, identifier]] # var1 variable is currently labeled 0\n",
    "    sentences.columns = ['sentence', identifier] # renaming var1\n",
    "    \n",
    "    if how == 'merge':\n",
    "        return sentences.merge(frame, left_on=identifier, right_on=identifier, how='outer', sort=False, suffixes=('_l','_r')) \n",
    "    elif how == 'nomerge': \n",
    "        return sentences\n",
    "    else: \n",
    "        return sentences.merge(frame, left_on=identifier, right_on=identifier, how='outer', sort=False, suffixes=('_l','_r')) \n",
    "\n",
    "\n",
    "def make_lowercase(comment):\n",
    "    return remove_newlines(comment.lower())\n",
    "\n",
    "\n",
    "# def tokenize_aslist(comment):\n",
    "#     comment = remove_newlines(comment)\n",
    "#     if comment == []:\n",
    "#         return None\n",
    "#     else: \n",
    "#         return [word for word in word_tokenize(remove_newlines(comment).lower()) if word not in mystops]\n",
    "\n",
    "    \n",
    "def tokenize(comment):\n",
    "    comment = remove_newlines(comment)\n",
    "    if comment == []:\n",
    "        return None\n",
    "    else: \n",
    "        return ' '.join([word for word in word_tokenize(remove_newlines(comment).lower()) if word not in mystops])\n",
    "\n",
    "    \n",
    "# def tokenize_stem_aslist(comment):\n",
    "#     tokens = word_tokenize(remove_newlines(comment).lower())\n",
    "#     if tokens == []:\n",
    "#         return None\n",
    "#     else: \n",
    "#         return[porter.stem(word) for word in tokens if word not in mystops]\n",
    "    \n",
    "def tokenize_stem(comment):\n",
    "    tokens = word_tokenize(remove_newlines(comment).lower())\n",
    "    if tokens == []:\n",
    "        return None\n",
    "    else: \n",
    "        return ' '.join([porter.stem(word) for word in tokens if word not in mystops])\n",
    "\n",
    "\n",
    "def remove_newlines(comment):    \n",
    "    return re.sub(r\"\\n\", \" \", comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import & sanity check"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "comments = pd.read_csv('/Users/kateliea/Documents/Insight/project/webscrapers/comments_smittenkitchen_100.csv', index_col=0)\n",
    "recipes = pd.read_csv('/Users/kateliea/Documents/Insight/project/webscrapers/recipes_smittenkitchen_100.csv', index_col=0)\n",
    "\n",
    "comments_classified = pd.read_csv('comments_classified_SK_filtered2000.csv', index_col=0)\n",
    "\n",
    "comments.columns, len(comments), len(recipes), recipes.numberofcomments.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a bit of cleaning - remove any comments where there is not comment text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comments = pd.read_csv('/Users/kateliea/Documents/Insight/project/webscrapers/comments_smittenkitchen_100.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "comments['commentID'] = comments.index\n",
    "\n",
    "# remove any comments with no comment text\n",
    "comments = comments[pd.notnull(comments.usercomment)]\n",
    "\n",
    "# replace NaN usernames with 'anon'\n",
    "comments['username'].fillna('anon', inplace=True)\n",
    "\n",
    "# sanity check\n",
    "# recipes.numberofcomments.sum(), len(comments), len(recipes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make new dataframe with sentences separated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_comments_data(frame):\n",
    "    # make sure commentIDs are unique ( = row identity)\n",
    "    frame['commentID'] = frame.index\n",
    "\n",
    "    # remove any frame with no comment text\n",
    "    frame = frame[pd.notnull(frame.usercomment)]\n",
    "\n",
    "    # replace NaN usernames with 'anon'\n",
    "    frame['username'].fillna('anon', inplace=True)\n",
    "\n",
    "    # \n",
    "    frame_sentences = separate_sentences(frame, 'commentID','usercomment',how='merge')\n",
    "    frame_sentences = frame_sentences.dropna()\n",
    "    frame_sentences = frame_sentences.drop([],axis=0)\n",
    "    \n",
    "    # tokenize data\n",
    "    frame_sentences['usercomment'] = frame_sentences.usercomment.apply(remove_newlines)\n",
    "    frame_sentences['usercomment_lower'] = frame_sentences.usercomment.apply(make_lowercase)\n",
    "\n",
    "    frame_sentences['tokens'] = frame_sentences.usercomment.apply(tokenize)\n",
    "    frame_sentences['tokens_stemmed'] = frame_sentences.usercomment.apply(tokenize_stem)\n",
    "    \n",
    "    frame_sentences['sentence_tokens'] = frame_sentences.sentence.apply(tokenize)\n",
    "    frame_sentences['sentence_tokens_stemmed'] = frame_sentences.sentence.apply(tokenize_stem)\n",
    "    frame_sentences = frame_sentences.dropna()\n",
    "    frame_sentences = frame_sentences.drop([],axis=0)\n",
    "\n",
    "#     frame_sentences['sentence_tokens_aslist'] = frame_sentences.sentence.apply(tokenize_aslist)\n",
    "#     frame_sentences = frame_sentences.dropna()\n",
    "#     frame_sentences = frame_sentences.drop([],axis=0)\n",
    "\n",
    "#     frame_sentences['sentence_tokens_stemmed_aslist'] = frame_sentences.sentence.apply(tokenize_stem_aslist)\n",
    "#     frame_sentences = frame_sentences.dropna()\n",
    "#     frame_sentences = frame_sentences.drop([],axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "comments_sentences = separate_sentences(comments, 'commentID','usercomment',how='merge')\n",
    "comments_sentences = comments_sentences.dropna()\n",
    "comments_sentences = comments_sentences.drop([],axis=0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "comments_sentences.columns"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "comments_sentences['usercomment'] = comments_sentences.usercomment.apply(remove_newlines)\n",
    "comments_sentences['usercomment_lower'] = comments_sentences.usercomment.apply(make_lowercase)\n",
    "\n",
    "comments_sentences['tokens'] = comments_sentences.usercomment.apply(tokenize)\n",
    "comments_sentences['tokens_stemmed'] = comments_sentences.usercomment.apply(tokenize_stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comments_sentences = pd.read_csv('comments_classified_SK_filtered2000.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1999, 15),\n",
       " Index(['category', 'sentence', 'commentID', 'child_id', 'children',\n",
       "        'comment_time', 'recipenumber', 'title', 'url', 'usercomment',\n",
       "        'username', 'usersite', 'usercomment_lower', 'tokens',\n",
       "        'tokens_stemmed'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_sentences.shape, comments.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "comments_sentences['sentence_tokens_aslist'] = comments_sentences.sentence.apply(tokenize_aslist)\n",
    "comments_sentences = comments_sentences.dropna()\n",
    "comments_sentences = comments_sentences.drop([],axis=0)\n",
    "\n",
    "comments_sentences['sentence_tokens_stemmed_aslist'] = comments_sentences.sentence.apply(tokenize_stem_aslist)\n",
    "comments_sentences = comments_sentences.dropna()\n",
    "comments_sentences = comments_sentences.drop([],axis=0)\n",
    "\n",
    "comments_sentences['sentence_tokens'] = comments_sentences.sentence.apply(tokenize)\n",
    "comments_sentences = comments_sentences.dropna()\n",
    "comments_sentences = comments_sentences.drop([],axis=0)\n",
    "\n",
    "comments_sentences['sentence_tokens_stemmed'] = comments_sentences.sentence.apply(tokenize_stem)\n",
    "comments_sentences = comments_sentences.dropna()\n",
    "comments_sentences = comments_sentences.drop([],axis=0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def join_list(elem): \n",
    "    return ' '.join(elem)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "comments_sentences['sentence_tokens'] = comments_sentences['sentence_tokens'].apply(join_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word relevancy - tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = comments_sentences.sentence_tokens_stemmed.as_matrix()\n",
    "target = comments_sentences.category.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1999, 2456)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X_train_counts = vectorizer.fit_transform(X_train)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1999, 2456)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer = TfidfTransformer(use_idf=True).fit(X_train_counts)\n",
    "\n",
    "X_train_tf = transformer.transform(X_train_counts)\n",
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1999,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = MultinomialNB().fit(X_train_tf, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test = comments_sentences.sentence_tokens_stemmed[:1999].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "comments_classified.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = comments.tokens_stemmed\n",
    "\n",
    "pca = decomposition.PCA(n_components=10)\n",
    "\n",
    "pca.fit(X)\n",
    "X_trans = pca.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
