{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; float:center}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"<style>.container { width:100% !important; float:center}</style>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import re\n",
    "from collections import Counter\n",
    "import datetime\n",
    "from itertools import groupby\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as skl\n",
    "# import scipy as sp\n",
    "# import scipy.stats as sps\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import wordcloud as wc\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "# from nltk import Text\n",
    "\n",
    "import time\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set_context('notebook')\n",
    "sns.set_style('ticks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set styles, stopwords, define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stops = set(stopwords.words('english'))\n",
    "    # using a set will make it faster to run through...\n",
    "\n",
    "punctuation = ['.',',',':','!',';','-','?','\"',\"'\",'(',')','—']   \n",
    "other = ['ive','ve', \"i've\", \"i'v\", 'i’ll', 'i’ve', 'i’v']  # 'deb','hideb','don','didn','twaittry','doesn','thank','heydeb',\n",
    "    \n",
    "stops_punc = set(stopwords.words('english') + punctuation)\n",
    "\n",
    "mystops = stopwords.words('english') + punctuation + other\n",
    "mystops_set = set(stopwords.words('english') + punctuation + other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize_sentences(comment):\n",
    "    return nltk.sent_tokenize(remove_newlines(comment.lower()))\n",
    "    \n",
    "    \n",
    "# def separate_sentences(frame, identifier, paragraph, how='merge'):\n",
    "#     sentences = pd.concat([pd.Series(row[identifier], tokenize_sentences((row[paragraph]))) for _, row in frame.iterrows()]).reset_index()\n",
    "#     sentences.columns = ['sentence', identifier]\n",
    "    \n",
    "#     if how == 'merge':\n",
    "#         return sentences.merge(frame, left_on=identifier, right_on=identifier, how='outer', sort=False, suffixes=('_l','_r')) \n",
    "#     elif how == 'nomerge': \n",
    "#         return sentences\n",
    "#     else: \n",
    "#         return sentences.merge(frame, left_on=identifier, right_on=identifier, how='outer', sort=False, suffixes=('_l','_r')) \n",
    "\n",
    "# slightly faster version\n",
    "def separate_sentences(frame, identifier, paragraph, how='merge'):\n",
    "    sentences = pd.DataFrame((tokenize_sentences(row[paragraph]) for _, row in frame.iterrows()), index=frame[identifier]).stack()\n",
    "    sentences = sentences.reset_index() [[0, identifier]] # var1 variable is currently labeled 0\n",
    "    sentences.columns = ['sentence', identifier] # renaming var1\n",
    "    \n",
    "    if how == 'merge':\n",
    "        return sentences.merge(frame, left_on=identifier, right_on=identifier, how='outer', sort=False, suffixes=('_l','_r')) \n",
    "    elif how == 'nomerge': \n",
    "        return sentences\n",
    "    else: \n",
    "        return sentences.merge(frame, left_on=identifier, right_on=identifier, how='outer', sort=False, suffixes=('_l','_r')) \n",
    "\n",
    "\n",
    "def make_lowercase(comment):\n",
    "    return remove_newlines(comment.lower())\n",
    "\n",
    "\n",
    "# def tokenize_aslist(comment):\n",
    "#     comment = remove_newlines(comment)\n",
    "#     if comment == []:\n",
    "#         return None\n",
    "#     else: \n",
    "#         return [word for word in word_tokenize(remove_newlines(comment).lower()) if word not in mystops]\n",
    "\n",
    "    \n",
    "def tokenize(comment):\n",
    "    comment = remove_newlines(comment)\n",
    "    if comment == []:\n",
    "        return None\n",
    "    else: \n",
    "        return ' '.join([word for word in word_tokenize(remove_newlines(comment).lower()) if word not in mystops])\n",
    "\n",
    "def ngram(comment):\n",
    "    comment = remove_newlines(comment)\n",
    "    if comment == []:\n",
    "        return None\n",
    "    else: \n",
    "        return [word for word in ngrams(remove_newlines(comment).lower().split(),gram) if word not in mystops]\n",
    "\n",
    "    \n",
    "# def tokenize_stem_aslist(comment):\n",
    "#     tokens = word_tokenize(remove_newlines(comment).lower())\n",
    "#     if tokens == []:\n",
    "#         return None\n",
    "#     else: \n",
    "#         return[porter.stem(word) for word in tokens if word not in mystops]\n",
    "    \n",
    "def tokenize_stem(comment):\n",
    "    tokens = word_tokenize(remove_newlines(comment).lower())\n",
    "    if tokens == []:\n",
    "        return None\n",
    "    else: \n",
    "        return ' '.join([porter.stem(word) for word in tokens if word not in mystops])\n",
    "\n",
    "\n",
    "def remove_newlines(comment):    \n",
    "    return re.sub(r\"\\n\", \" \", comment)\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_comments_data(frame):\n",
    "    # make sure commentIDs are unique ( = row identity)\n",
    "    frame.loc[:,'commentID'] = frame.index\n",
    "\n",
    "    # remove any frame with no comment text\n",
    "    frame = frame.loc[pd.notnull(frame['usercomment']),:]\n",
    "\n",
    "    # replace NaN usernames with 'anon'\n",
    "    frame.loc[:,'username'].fillna('anon', inplace=True)\n",
    "\n",
    "    # \n",
    "    frame2 = separate_sentences(frame, 'commentID','usercomment',how='merge')\n",
    "    frame2.dropna(inplace=True)\n",
    "    frame2.drop([],axis=0, inplace=True)\n",
    "    \n",
    "    # tokenize data\n",
    "    frame2.loc[:,'usercomment'] = frame2.loc[:,'usercomment'].apply(remove_newlines)\n",
    "    frame2.loc[:,'usercomment_lower'] = frame2.loc[:,'usercomment'].apply(make_lowercase)\n",
    "\n",
    "    frame2.loc[:,'tokens'] = frame2.loc[:,'usercomment'].apply(tokenize)\n",
    "    frame2.loc[:,'tokens_stemmed'] = frame2.loc[:,'usercomment'].apply(tokenize_stem)\n",
    "    \n",
    "    frame2.loc[:,'sentence_tokens'] = frame2.loc[:,'sentence'].apply(tokenize)\n",
    "    frame2.loc[:,'sentence_tokens_stemmed'] = frame2.loc[:,'sentence'].apply(tokenize_stem)\n",
    "    \n",
    "    gram = 2\n",
    "    comments_classified.loc[:,'sentence_bigrams'] = comments_classified.loc[:,'sentence'].apply(ngram)\n",
    "    gram = 3\n",
    "    comments_classified.loc[:,'sentence_trigrams'] = comments_classified.loc[:,'sentence'].apply(ngram)\n",
    "    \n",
    "    frame2.dropna(inplace=True)\n",
    "    frame2.drop([],axis=0, inplace=True)\n",
    "\n",
    "#     frame2['sentence_tokens_aslist'] = frame2.sentence.apply(tokenize_aslist)\n",
    "#     frame2 = frame2.dropna()\n",
    "#     frame2 = frame2.drop([],axis=0)\n",
    "\n",
    "#     frame2['sentence_tokens_stemmed_aslist'] = frame2.sentence.apply(tokenize_stem_aslist)\n",
    "#     frame2 = frame2.dropna()\n",
    "#     frame2 = frame2.drop([],axis=0)\n",
    "\n",
    "    return frame, frame2 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import & sanity check"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "comments = pd.read_csv('/Users/kateliea/Documents/Insight/project/webscrapers/comments_smittenkitchen_100.csv', index_col=0)\n",
    "recipes = pd.read_csv('/Users/kateliea/Documents/Insight/project/webscrapers/recipes_smittenkitchen_100.csv', index_col=0)\n",
    "\n",
    "comments_classified = pd.read_csv('comments_classified_SK_filtered2000.csv', index_col=0)\n",
    "\n",
    "comments.columns, len(comments), len(recipes), recipes.numberofcomments.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comments = pd.read_csv('/Users/kateliea/Documents/Insight/project/webscrapers/comments_smittenkitchen_100.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "comments.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comments_classified = pd.read_csv('comments_classified_SK_filtered2000.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'comments_only' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-35e1601eaa7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcomments_only\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'comments_only' is not defined"
     ]
    }
   ],
   "source": [
    "comments_only.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make new dataframe with sentences separated, tokenize everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "comments_only, comments_with_sentences = preprocess_comments_data(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "comments_with_sentences.to_csv('comments_with_sentences.csv'), comments_only.to_csv('comments_only.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gram = 2\n",
    "comments_classified.loc[:,'sentence_bigrams'] = comments_classified.loc[:,'sentence'].apply(ngram)\n",
    "gram = 3\n",
    "comments_classified.loc[:,'sentence_trigrams'] = comments_classified.loc[:,'sentence'].apply(ngram)\n",
    "\n",
    "comments_classified.loc[:,'sentence_tokens'] = comments_classified.loc[:,'sentence'].apply(tokenize)\n",
    "comments_classified.loc[:,'sentence_tokens_stemmed'] = comments_classified.loc[:,'sentence'].apply(tokenize_stem)\n",
    "\n",
    "comments_classified.dropna(inplace=True)\n",
    "comments_classified.drop([],axis=0, inplace=True)\n",
    "\n",
    "comments_classified.to_csv('comments_classified_SK_filtered2000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comments_classified.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define test, train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comments_classified.to_csv('comments_classified_SK_filtered2000_additional.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "comments_classified.loc[:,'category_label'] = le.fit_transform(comments_classified.category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "comments_classified.category.unique()\n",
    "# comments_classified.category.replace('try','other',inplace=True)\n",
    "# comments_classified.category.replace('addition','suggestion',inplace=True)\n",
    "# comments_classified.category.replace('subtraction','substitution',inplace=True)\n",
    "# comments_classified.category.replace('related','other',inplace=True)\n",
    "# comments_classified.category.replace('question','other',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = comments_classified[comments_classified.category != 'other'].sentence #\\.as_matrix()\n",
    "# target = comments_classified.category.as_matrix()\n",
    "target = comments_classified[comments_classified.category != 'other'].category_label #.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test = comments_with_sentences.loc[2000:, 'sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "le.inverse_transform([2,1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word relevancy - counter, tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(2,3), stop_words=mystops)\n",
    "\n",
    "X_train_counts = vectorizer.fit_transform(X_train)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "\n",
    "X_train_tf = transformer.transform(X_train_counts)\n",
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "classifier = MultinomialNB().fit(X_train_tf, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = BernoulliNB().fit(X_train_counts, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test_counts = vectorizer.transform(X_test)\n",
    "X_test_tf = transformer.transform(X_test_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted = classifier.predict(X_test_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for doc, category in zip(X_test, predicted):\n",
    "    print('%r => %s' % (doc, category) if category != 'other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "comments_classified.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = decomposition.PCA(n_components=10)\n",
    "\n",
    "pca.fit(X_train_count)\n",
    "X_trans = pca.transform(X_train_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_target(df, target_column):\n",
    "    \"\"\"Add column to df with integers for the target.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    df -- pandas DataFrame.\n",
    "    target_column -- column to map to int, producing\n",
    "                     new Target column.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_mod -- modified DataFrame.\n",
    "    targets -- list of target names.\n",
    "    \"\"\"\n",
    "    df_mod = df.copy()\n",
    "    targets = df_mod[target_column].unique()\n",
    "    map_to_int = {name: n for n, name in enumerate(targets)}\n",
    "    df_mod[\"Target\"] = df_mod[target_column].replace(map_to_int)\n",
    "\n",
    "    return (df_mod, targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x,y = encode_target(comments_classified, 'category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "comments_classified.loc[comments_classified.sentence.str.contains('instead of') == True, 'category'] = 'substitution'\n",
    "\n",
    "# comments_classified"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "comments_classified[comments_classified.sentence.str.contains('perfect for') == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree_classifier = DecisionTreeClassifier(max_depth=3, max_features=10)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_fit = tree_classifier.fit(X_train_counts, target)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "comments_classified.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "comments.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
