{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; float:center}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"<style>.container { width:100% !important; float:center}</style>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "# %matplotlib inline\n",
    "\n",
    "# sns.set_context('notebook')\n",
    "# sns.set_style('ticks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set styles, stopwords, define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stops = set(stopwords.words('english'))\n",
    "    # using a set will make it faster to run through...\n",
    "\n",
    "punctuation = ['.',',',':','!',';','-','?','\"',\"'\",'(',')','—']   \n",
    "other = ['ive','ve', \"i've\", \"i'v\", 'i’ll', 'i’ve', 'i’v']  # 'deb','hideb','don','didn','twaittry','doesn','thank','heydeb',\n",
    "    \n",
    "stops_punc = set(stopwords.words('english') + punctuation)\n",
    "\n",
    "mystops = stopwords.words('english') + punctuation + other\n",
    "mystops_set = set(stopwords.words('english') + punctuation + other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize_sentences(comment):\n",
    "    return nltk.sent_tokenize(remove_newlines(comment.lower()))\n",
    "    \n",
    "    \n",
    "# def separate_sentences(frame, identifier, paragraph, how='merge'):\n",
    "#     sentences = pd.concat([pd.Series(row[identifier], tokenize_sentences((row[paragraph]))) for _, row in frame.iterrows()]).reset_index()\n",
    "#     sentences.columns = ['sentence', identifier]\n",
    "    \n",
    "#     if how == 'merge':\n",
    "#         return sentences.merge(frame, left_on=identifier, right_on=identifier, how='outer', sort=False, suffixes=('_l','_r')) \n",
    "#     elif how == 'nomerge': \n",
    "#         return sentences\n",
    "#     else: \n",
    "#         return sentences.merge(frame, left_on=identifier, right_on=identifier, how='outer', sort=False, suffixes=('_l','_r')) \n",
    "\n",
    "# slightly faster version\n",
    "def separate_sentences(frame, identifier, paragraph, how='merge'):\n",
    "    sentences = pd.DataFrame((tokenize_sentences(row[paragraph]) for _, row in frame.iterrows()), index=frame[identifier]).stack()\n",
    "    sentences = sentences.reset_index() [[0, identifier]] # var1 variable is currently labeled 0\n",
    "    sentences.columns = ['sentence', identifier] # renaming var1\n",
    "    \n",
    "    if how == 'merge':\n",
    "        return sentences.merge(frame, left_on=identifier, right_on=identifier, how='outer', sort=False, suffixes=('_l','_r')) \n",
    "    elif how == 'nomerge': \n",
    "        return sentences\n",
    "    else: \n",
    "        return sentences.merge(frame, left_on=identifier, right_on=identifier, how='outer', sort=False, suffixes=('_l','_r')) \n",
    "\n",
    "\n",
    "def make_lowercase(comment):\n",
    "    return remove_newlines(comment.lower())\n",
    "\n",
    "\n",
    "# def tokenize_aslist(comment):\n",
    "#     comment = remove_newlines(comment)\n",
    "#     if comment == []:\n",
    "#         return None\n",
    "#     else: \n",
    "#         return [word for word in word_tokenize(remove_newlines(comment).lower()) if word not in mystops]\n",
    "\n",
    "    \n",
    "def tokenize(comment):\n",
    "    comment = remove_newlines(comment)\n",
    "    if comment == []:\n",
    "        return None\n",
    "    else: \n",
    "        return ' '.join([word for word in word_tokenize(remove_newlines(comment).lower()) if word not in mystops])\n",
    "\n",
    "def ngram(comment):\n",
    "    comment = remove_newlines(comment)\n",
    "    if comment == []:\n",
    "        return None\n",
    "    else: \n",
    "        return [word for word in ngrams(remove_newlines(comment).lower().split(),gram) if word not in mystops]\n",
    "\n",
    "    \n",
    "# def tokenize_stem_aslist(comment):\n",
    "#     tokens = word_tokenize(remove_newlines(comment).lower())\n",
    "#     if tokens == []:\n",
    "#         return None\n",
    "#     else: \n",
    "#         return[porter.stem(word) for word in tokens if word not in mystops]\n",
    "    \n",
    "def tokenize_stem(comment):\n",
    "    tokens = word_tokenize(remove_newlines(comment).lower())\n",
    "    if tokens == []:\n",
    "        return None\n",
    "    else: \n",
    "        return ' '.join([porter.stem(word) for word in tokens if word not in mystops])\n",
    "\n",
    "\n",
    "def remove_newlines(comment):    \n",
    "    return re.sub(r\"\\n\", \" \", comment)\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_comments_data(frame):\n",
    "    # make sure commentIDs are unique ( = row identity)\n",
    "    frame.loc[:,'commentID'] = frame.index\n",
    "\n",
    "    # remove any frame with no comment text\n",
    "    frame = frame.loc[pd.notnull(frame['usercomment']),:]\n",
    "\n",
    "    # replace NaN usernames with 'anon'\n",
    "    frame.loc[:,'username'].fillna('anon', inplace=True)\n",
    "\n",
    "    # tokenize data\n",
    "    frame.loc[:,'usercomment'] = frame.loc[:,'usercomment'].apply(remove_newlines)\n",
    "    frame.loc[:,'usercomment_lower'] = frame.loc[:,'usercomment'].apply(make_lowercase)\n",
    "\n",
    "    frame.loc[:,'tokens'] = frame.loc[:,'usercomment'].apply(tokenize)\n",
    "    frame.loc[:,'tokens_stemmed'] = frame.loc[:,'usercomment'].apply(tokenize_stem)\n",
    "    \n",
    "    frame.dropna(inplace=True)\n",
    "    frame.drop([],axis=0, inplace=True)\n",
    "\n",
    "    \n",
    "    frame2 = separate_sentences(frame, 'commentID','usercomment',how='merge')\n",
    "    frame2.dropna(inplace=True)\n",
    "    frame2.drop([],axis=0, inplace=True)\n",
    "    \n",
    "\n",
    "    frame2.loc[:,'sentence_tokens'] = frame2.loc[:,'sentence'].apply(tokenize)\n",
    "    frame2.loc[:,'sentence_tokens_stemmed'] = frame2.loc[:,'sentence'].apply(tokenize_stem)\n",
    "    \n",
    "    gram = 2\n",
    "    comments_classified.loc[:,'sentence_bigrams'] = comments_classified.loc[:,'sentence'].apply(ngram)\n",
    "    gram = 3\n",
    "    comments_classified.loc[:,'sentence_trigrams'] = comments_classified.loc[:,'sentence'].apply(ngram)\n",
    "    \n",
    "    frame2.dropna(inplace=True)\n",
    "    frame2.drop([],axis=0, inplace=True)\n",
    "\n",
    "#     frame2['sentence_tokens_aslist'] = frame2.sentence.apply(tokenize_aslist)\n",
    "#     frame2 = frame2.dropna()\n",
    "#     frame2 = frame2.drop([],axis=0)\n",
    "\n",
    "#     frame2['sentence_tokens_stemmed_aslist'] = frame2.sentence.apply(tokenize_stem_aslist)\n",
    "#     frame2 = frame2.dropna()\n",
    "#     frame2 = frame2.drop([],axis=0)\n",
    "\n",
    "    return frame, frame2 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import & sanity check"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "comments = pd.read_csv('/Users/kateliea/Documents/Insight/project/webscrapers/comments_smittenkitchen_100.csv', index_col=0)\n",
    "recipes = pd.read_csv('/Users/kateliea/Documents/Insight/project/webscrapers/recipes_smittenkitchen_100.csv', index_col=0)\n",
    "\n",
    "comments_classified = pd.read_csv('comments_classified_SK_filtered2000.csv', index_col=0)\n",
    "\n",
    "comments.columns, len(comments), len(recipes), recipes.numberofcomments.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "comments = pd.read_csv('/Users/kateliea/Documents/Insight/project/webscrapers/comments_smittenkitchen_all.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['child_id', 'children', 'commentID', 'comment_time', 'recipenumber',\n",
       "       'title', 'url', 'usercomment', 'username', 'usersite'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments.columns"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "comments_classified = pd.read_csv('comments_classified_SK_filtered2000.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "comments_classified.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make new dataframe with sentences separated, tokenize everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kateliea/anaconda/envs/py3/lib/python3.5/site-packages/pandas/core/generic.py:3191: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n",
      "/Users/kateliea/anaconda/envs/py3/lib/python3.5/site-packages/pandas/core/indexing.py:465: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n",
      "/Users/kateliea/anaconda/envs/py3/lib/python3.5/site-packages/pandas/core/indexing.py:288: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "/Users/kateliea/anaconda/envs/py3/lib/python3.5/site-packages/ipykernel/__main__.py:94: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/kateliea/anaconda/envs/py3/lib/python3.5/site-packages/ipykernel/__main__.py:95: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'comments_classified' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-78a18688fe2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcomments_only\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomments_with_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_comments_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-9985c0f90227>\u001b[0m in \u001b[0;36mpreprocess_comments_data\u001b[0;34m(frame)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0mgram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0mcomments_classified\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'sentence_bigrams'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomments_classified\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'sentence'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m     \u001b[0mgram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0mcomments_classified\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'sentence_trigrams'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomments_classified\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'sentence'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'comments_classified' is not defined"
     ]
    }
   ],
   "source": [
    "comments_only, comments_with_sentences = preprocess_comments_data(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "comments_with_sentences.to_csv('comments_with_sentences_all.csv'), comments_only.to_csv('comments_only_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "comments_only.shape, comments_with_sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gram = 2\n",
    "comments_classified.loc[:,'sentence_bigrams'] = comments_classified.loc[:,'sentence'].apply(ngram)\n",
    "gram = 3\n",
    "comments_classified.loc[:,'sentence_trigrams'] = comments_classified.loc[:,'sentence'].apply(ngram)\n",
    "\n",
    "comments_classified.loc[:,'sentence_tokens'] = comments_classified.loc[:,'sentence'].apply(tokenize)\n",
    "comments_classified.loc[:,'sentence_tokens_stemmed'] = comments_classified.loc[:,'sentence'].apply(tokenize_stem)\n",
    "\n",
    "comments_classified.dropna(inplace=True)\n",
    "comments_classified.drop([],axis=0, inplace=True)\n",
    "\n",
    "comments_classified.to_csv('comments_classified_SK_filtered2000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comments_classified.head()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
