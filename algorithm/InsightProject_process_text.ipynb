{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; float:center}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"<style>.container { width:100% !important; float:center}</style>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set styles, stopwords, define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stops = set(stopwords.words('english'))\n",
    "    # using a set will make it faster to run through...\n",
    "\n",
    "punctuation = ['.',',',':','!',';','-','?','\"',\"'\",'(',')','—']   \n",
    "other = ['ive','ve', \"i've\", \"i'v\", 'i’ll', 'i’ve', 'i’v']  # 'deb','hideb','don','didn','twaittry','doesn','thank','heydeb',\n",
    "    \n",
    "stops_punc = set(stopwords.words('english') + punctuation)\n",
    "\n",
    "mystops = stopwords.words('english') + punctuation + other\n",
    "mystops_set = set(stopwords.words('english') + punctuation + other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## keyword dictionary for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# subs = ['had trouble','had issue','exchange','recommend','change','easier','gluten free','gluten-free','vegan','vegetarian','easier','replace with','replac','adjust','suggest','swap','switch','instead of','in place of','substitute','replace','i use','to make it','customiz','adjust','instead','exchang','opted to use','better with']\n",
    "# add = ['will ad','will do','next time','going to add','add more','better with','includ','i up','should have ad']\n",
    "# omit = ['omit','leave out','remov','left out','eliminat','add less','forgot','decreas','dispensed with','delet','reduc','should have left out']\n",
    "\n",
    "subs = ['exchange','recommend','easier','replace with','substitute','replace','i use','to make it','instead','exchang']\n",
    "\n",
    "add = ['will ad','will do','next time','going to add','add more','better with','includ','i up','should have ad']\n",
    "\n",
    "omit = ['omit','leave out','remov','eliminat','add less','forgot','decreas','should have left out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize_sentences(comment):\n",
    "    return nltk.sent_tokenize(remove_newlines(comment.lower()))\n",
    "    \n",
    "# slightly faster version\n",
    "def separate_sentences(frame, identifier, paragraph, how='merge'):\n",
    "    sentences = pd.DataFrame((tokenize_sentences(row[paragraph]) for _, row in frame.iterrows()), index=frame[identifier]).stack()\n",
    "    sentences = sentences.reset_index() [[0, identifier]] # var1 variable is currently labeled 0\n",
    "    sentences.columns = ['sentence', identifier] # renaming var1\n",
    "    \n",
    "    if how == 'merge':\n",
    "        return sentences.merge(frame, left_on=identifier, right_on=identifier, how='outer', sort=False, suffixes=('_l','_r')) \n",
    "    elif how == 'nomerge': \n",
    "        return sentences\n",
    "    else: \n",
    "        return sentences.merge(frame, left_on=identifier, right_on=identifier, how='outer', sort=False, suffixes=('_l','_r')) \n",
    "\n",
    "\n",
    "def make_lowercase(comment):\n",
    "    return remove_newlines(comment.lower())\n",
    "\n",
    "   \n",
    "def tokenize(comment):\n",
    "    comment = remove_newlines(comment)\n",
    "    if comment == []:\n",
    "        return None\n",
    "    else: \n",
    "        return ' '.join([word for word in word_tokenize(remove_newlines(comment).lower())])\n",
    "\n",
    "def ngram(comment):\n",
    "    comment = remove_newlines(comment)\n",
    "    if comment == []:\n",
    "        return None\n",
    "    else: \n",
    "        return [word for word in ngrams(remove_newlines(comment).lower().split(),gram)]\n",
    "    \n",
    "def tokenize_stem(comment):\n",
    "    tokens = word_tokenize(remove_newlines(comment).lower())\n",
    "    if tokens == []:\n",
    "        return None\n",
    "    else: \n",
    "        return ' '.join([porter.stem(word) for word in tokens])\n",
    "\n",
    "def tokenize_stem_stops(comment):\n",
    "    tokens = word_tokenize(remove_newlines(comment).lower())\n",
    "    if tokens == []:\n",
    "        return None\n",
    "    else: \n",
    "        return ' '.join([porter.stem(word) for word in tokens if word not in mystops])\n",
    "\n",
    "\n",
    "def remove_newlines(comment):    \n",
    "    return re.sub(r\"\\n\", \" \", comment)\n",
    "\n",
    "\n",
    "def preprocess_comments_data(frame):\n",
    "    # make sure commentIDs are unique ( = row identity)\n",
    "    frame.loc[:,'commentID'] = frame.index\n",
    "\n",
    "    # remove any frame with no comment text\n",
    "    frame = frame.loc[pd.notnull(frame['usercomment']),:]\n",
    "\n",
    "    # replace NaN usernames with 'anon'\n",
    "    frame.loc[:,'username'].fillna('anon', inplace=True)\n",
    "\n",
    "    # tokenize data\n",
    "    frame.loc[:,'usercomment'] = frame.loc[:,'usercomment'].apply(remove_newlines)\n",
    "    frame.loc[:,'usercomment_lower'] = frame.loc[:,'usercomment'].apply(make_lowercase)\n",
    "\n",
    "    frame.loc[:,'tokens'] = frame.loc[:,'usercomment'].apply(tokenize)\n",
    "    frame.loc[:,'tokens_stemmed'] = frame.loc[:,'usercomment'].apply(tokenize_stem)\n",
    "    \n",
    "    frame.dropna(inplace=True)\n",
    "    frame.drop([],axis=0, inplace=True)\n",
    "\n",
    "    \n",
    "    frame2 = separate_sentences(frame, 'commentID','usercomment',how='merge')\n",
    "    frame2.dropna(inplace=True)\n",
    "    frame2.drop([],axis=0, inplace=True)\n",
    "    \n",
    "\n",
    "    frame2.loc[:,'sentence_tokens'] = frame2.loc[:,'sentence'].apply(tokenize)\n",
    "    frame2.loc[:,'sentence_tokens_stemmed_stops'] = frame2.loc[:,'sentence'].apply(tokenize_stem_stops)\n",
    "    frame2.loc[:,'sentence_tokens_stemmed'] = frame2.loc[:,'sentence'].apply(tokenize_stem)\n",
    "    \n",
    "#     gram = 2\n",
    "#     frame2.loc[:,'sentence_bigrams'] = frame2.loc[:,'sentence'].apply(ngram)\n",
    "#     gram = 3\n",
    "#     frame2.loc[:,'sentence_trigrams'] = frame2.loc[:,'sentence'].apply(ngram)\n",
    "    \n",
    "    frame2.dropna(inplace=True)\n",
    "    frame2.drop([],axis=0, inplace=True)\n",
    "\n",
    "    return frame, frame2 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import & sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['child_id', 'children', 'commentID', 'comment_time', 'recipenumber',\n",
       "       'title', 'url', 'usercomment', 'username', 'usersite',\n",
       "       'usercomment_lower', 'tokens', 'tokens_stemmed'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname = 'training_data'\n",
    "comments = pd.read_csv('/Users/kateliea/Documents/Insight/project/data/partial_SK_data/comments_only_100.csv', index_col=0)\n",
    "comments.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make new dataframe with sentences separated, tokenize everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "comments_only, comments_with_sentences = preprocess_comments_data(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sentence', 'commentID', 'child_id', 'children', 'comment_time',\n",
       "       'recipenumber', 'title', 'url', 'usercomment', 'username', 'usersite',\n",
       "       'usercomment_lower', 'tokens', 'tokens_stemmed', 'sentence_tokens',\n",
       "       'sentence_tokens_stemmed_stops', 'sentence_tokens_stemmed'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_with_sentences.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classification of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "other, 91915; additions, 1053; substitutions, 3942; omissions, 406\n",
      "Index(['sentence', 'commentID', 'child_id', 'children', 'comment_time',\n",
      "       'recipenumber', 'title', 'url', 'usercomment', 'username', 'usersite',\n",
      "       'usercomment_lower', 'tokens', 'tokens_stemmed', 'sentence_tokens',\n",
      "       'sentence_tokens_stemmed_stops', 'sentence_tokens_stemmed', 'category'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "comments_with_sentences['category'] = 'other'\n",
    "\n",
    "for phrase in omit: \n",
    "    comments_with_sentences.loc[comments_with_sentences.sentence.str.contains(phrase) == True, 'category'] = 'omission'\n",
    "    \n",
    "for phrase in subs: \n",
    "    comments_with_sentences.loc[comments_with_sentences.sentence.str.contains(phrase) == True, 'category'] = 'substitution'\n",
    "    \n",
    "for phrase in add: \n",
    "    comments_with_sentences.loc[comments_with_sentences.sentence.str.contains(phrase) == True, 'category'] = 'addition'\n",
    "    \n",
    "print('other, %i; additions, %i; substitutions, %i; omissions, %i' % (comments_with_sentences[comments_with_sentences.category == 'other'].sentence.count(), comments_with_sentences[comments_with_sentences.category == 'addition'].sentence.count(), comments_with_sentences[comments_with_sentences.category == 'substitution'].sentence.count(), comments_with_sentences[comments_with_sentences.category == 'omission'].sentence.count()))\n",
    "\n",
    "print(comments_with_sentences.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['other', 'helpful'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_with_sentences = comments_with_sentences.dropna()\n",
    "\n",
    "comments_with_sentences['category_simple'] = comments_with_sentences.loc[:,'category']\n",
    "comments_with_sentences['category_simple'] = comments_with_sentences.category_simple.replace(['addition','substitution','omission'],'helpful')\n",
    "\n",
    "comments_with_sentences.category_simple.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for name in ['sent_neg','sent_neu','sent_pos','sent_compound']:\n",
    "    comments_with_sentences[name] = 0\n",
    "\n",
    "for i, sentence in enumerate(comments_with_sentences.loc[:,'sentence']): \n",
    "    polarity = SentimentIntensityAnalyzer().polarity_scores(sentence)\n",
    "    comments_with_sentences.loc[i, 'sent_neg'] = polarity['neg']\n",
    "    comments_with_sentences.loc[i, 'sent_neu'] = polarity['neu']\n",
    "    comments_with_sentences.loc[i, 'sent_pos'] = polarity['pos']\n",
    "    comments_with_sentences.loc[i, 'sent_compound'] = polarity['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def analyze_sentiment(sentence):\n",
    "    polarity = SentimentIntensityAnalyzer().polarity_scores(sentence)\n",
    "#     comments_with_sentences.loc[i, 'sent_neg'] = polarity['neg']\n",
    "#     comments_with_sentences.loc[i, 'sent_neu'] = polarity['neu']\n",
    "#     comments_with_sentences.loc[i, 'sent_pos'] = polarity['pos']\n",
    "#     comments_with_sentences.loc[i, 'sent_compound'] = polarity['compound']\n",
    "        return polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fname = 'training_data'\n",
    "comments_with_sentences.to_csv(fname+'.csv') #, comments_only.to_csv(fname+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
